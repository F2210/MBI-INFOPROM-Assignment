{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc43439a",
   "metadata": {},
   "source": [
    "# Process Mining with PM4Py: Inductive Miner Variants\n",
    "\n",
    "This notebook demonstrates how to create different process models using various inductive miner implementations available in PM4Py. We'll use the filtered XES logs that were created during the preprocessing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b8a37f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pm4py\n",
    "import matplotlib.pyplot as plt\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "from pm4py.visualization.process_tree import visualizer as pt_visualizer\n",
    "from pm4py.visualization.petri_net import visualizer as pn_visualizer\n",
    "from pm4py.algo.evaluation.replay_fitness import algorithm as replay_fitness\n",
    "from pm4py.algo.evaluation.precision import algorithm as precision_evaluator\n",
    "from pm4py.algo.evaluation.generalization import algorithm as generalization_evaluator\n",
    "from pm4py.algo.evaluation.simplicity import algorithm as simplicity_evaluator\n",
    "\n",
    "# Set up the path to the data folder\n",
    "data_folder = './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4d78d7",
   "metadata": {},
   "source": [
    "## 1. Helper Functions\n",
    "\n",
    "Let's define some helper functions to load logs, discover models, visualize them, and evaluate their quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9344e986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_log(log_file_path):\n",
    "    \"\"\"\n",
    "    Load an XES log file\n",
    "    \n",
    "    Args:\n",
    "        log_file_path: Path to the XES file\n",
    "        \n",
    "    Returns:\n",
    "        PM4Py event log object\n",
    "    \"\"\"\n",
    "    print(f\"Loading log: {log_file_path}\")\n",
    "    log = xes_importer.apply(log_file_path)\n",
    "    print(f\"Log loaded with {len(log)} cases and {sum(len(case) for case in log)} events\")\n",
    "    return log\n",
    "\n",
    "def discover_models(log, variant_name, variant):\n",
    "    \"\"\"\n",
    "    Discover process models using the specified algorithm variant\n",
    "    \n",
    "    Args:\n",
    "        log: PM4Py event log\n",
    "        variant_name: String name of the variant (for display)\n",
    "        variant: The actual algorithm variant to use\n",
    "        \n",
    "    Returns:\n",
    "        Tuple containing process tree and Petri net with initial and final markings\n",
    "    \"\"\"\n",
    "    print(f\"\\nDiscovering models using {variant_name}...\")\n",
    "    \n",
    "    # Discover process tree\n",
    "    process_tree = inductive_miner.apply(log, variant=variant)\n",
    "    \n",
    "    # Convert to Petri net for visualization and conformance checking\n",
    "    net, initial_marking, final_marking = pm4py.convert_to_petri_net(process_tree)\n",
    "    \n",
    "    return process_tree, net, initial_marking, final_marking\n",
    "\n",
    "def visualize_models(process_tree, net, initial_marking, final_marking, variant_name, output_folder=None):\n",
    "    \"\"\"\n",
    "    Visualize both process tree and Petri net models\n",
    "    \n",
    "    Args:\n",
    "        process_tree: The process tree model\n",
    "        net: The Petri net\n",
    "        initial_marking: Initial marking for the Petri net\n",
    "        final_marking: Final marking for the Petri net\n",
    "        variant_name: String name of the variant (for display and filenames)\n",
    "        output_folder: Folder to save visualizations (if None, just display)\n",
    "    \"\"\"\n",
    "    # Visualize process tree\n",
    "    pt_gviz = pt_visualizer.apply(process_tree)\n",
    "    \n",
    "    # Visualize Petri net\n",
    "    parameters = {pn_visualizer.Variants.WO_DECORATION.value.Parameters.FORMAT: \"png\"}\n",
    "    pn_gviz = pn_visualizer.apply(net, initial_marking, final_marking, parameters=parameters)\n",
    "\n",
    "    # Display visualizations\n",
    "    if output_folder:\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        pt_visualizer.save(pt_gviz, os.path.join(output_folder, f\"process_tree_{variant_name}.png\"))\n",
    "        pn_visualizer.save(pn_gviz, os.path.join(output_folder, f\"petri_net_{variant_name}.png\"))\n",
    "        print(f\"Visualizations saved to {output_folder}\")\n",
    "    else:\n",
    "        pt_visualizer.view(pt_gviz)\n",
    "        pn_visualizer.view(pn_gviz)\n",
    "\n",
    "def evaluate_model(log, net, initial_marking, final_marking, variant_name):\n",
    "    \"\"\"\n",
    "    Evaluate a process model using the four quality dimensions\n",
    "    \n",
    "    Args:\n",
    "        log: PM4Py event log\n",
    "        net: The Petri net\n",
    "        initial_marking: Initial marking for the Petri net\n",
    "        final_marking: Final marking for the Petri net\n",
    "        variant_name: String name of the variant (for display)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating {variant_name} model...\")\n",
    "    \n",
    "    # Compute fitness using token-based replay\n",
    "    fitness = replay_fitness.evaluate(log, net, initial_marking, final_marking, \n",
    "                                    variant=replay_fitness.Variants.TOKEN_BASED)\n",
    "    \n",
    "    # Compute precision using ETConformance\n",
    "    precision = precision_evaluator.evaluate(log, net, initial_marking, final_marking,\n",
    "                                           variant=precision_evaluator.Variants.ETCONFORMANCE_TOKEN)\n",
    "    \n",
    "    # Compute generalization\n",
    "    generalization = generalization_evaluator.evaluate(log, net, initial_marking, final_marking)\n",
    "    \n",
    "    # Compute simplicity\n",
    "    simplicity = simplicity_evaluator.evaluate(net)\n",
    "    \n",
    "    metrics = {\n",
    "        'variant': variant_name,\n",
    "        'fitness': fitness['average_trace_fitness'],\n",
    "        'precision': precision,\n",
    "        'generalization': generalization,\n",
    "        'simplicity': simplicity\n",
    "    }\n",
    "    \n",
    "    print(f\"Fitness: {metrics['fitness']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Generalization: {metrics['generalization']:.4f}\")\n",
    "    print(f\"Simplicity: {metrics['simplicity']:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def process_group(log_name, output_folder=None):\n",
    "    \"\"\"\n",
    "    Process a log file using different inductive miner variants\n",
    "    \n",
    "    Args:\n",
    "        log_name: Name of the log file (without path)\n",
    "        output_folder: Folder to save visualizations\n",
    "    \n",
    "    Returns:\n",
    "        List of evaluation metrics for each variant\n",
    "    \"\"\"\n",
    "    log_path = os.path.join(data_folder, log_name)\n",
    "    \n",
    "    # If output folder is specified, create a subfolder for this log\n",
    "    if output_folder:\n",
    "        group_output_folder = os.path.join(output_folder, log_name.replace('.xes', ''))\n",
    "        os.makedirs(group_output_folder, exist_ok=True)\n",
    "    else:\n",
    "        group_output_folder = None\n",
    "    \n",
    "    # Load the log\n",
    "    log = load_log(log_path)\n",
    "    \n",
    "    # List of variants to try\n",
    "    variants = [\n",
    "        ('IM', inductive_miner.Variants.IM),\n",
    "        ('IMf', inductive_miner.Variants.IMf),\n",
    "        ('IMd', inductive_miner.Variants.IMd)\n",
    "    ]\n",
    "    \n",
    "    all_metrics = []\n",
    "    \n",
    "    # Process each variant\n",
    "    for variant_name, variant in variants:\n",
    "        # Discover models\n",
    "        process_tree, net, initial_marking, final_marking = discover_models(log, variant_name, variant)\n",
    "        \n",
    "        # Visualize models\n",
    "        visualize_models(process_tree, net, initial_marking, final_marking, variant_name, group_output_folder)\n",
    "        \n",
    "        # Evaluate model\n",
    "        metrics = evaluate_model(log, net, initial_marking, final_marking, variant_name)\n",
    "        all_metrics.append(metrics)\n",
    "    \n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844d82b6",
   "metadata": {},
   "source": [
    "## 2. Analyze Each Group\n",
    "\n",
    "Now let's process each filtered log in the data folder. We'll discover process models using different inductive miner variants for each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8793023f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 XES files in ./data:\n",
      "  - group_consignment.xes\n",
      "  - group_2_way.xes\n",
      "  - group_3_way_after.xes\n",
      "  - group_3_way_before.xes\n",
      "\n",
      "==================================================\n",
      "Processing group_consignment.xes\n",
      "==================================================\n",
      "Loading log: ./data/group_consignment.xes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b96871c21bd445aa71eac3af3f4bffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/11698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loaded with 11698 cases and 26953 events\n",
      "\n",
      "Discovering models using IM...\n",
      "Visualizations saved to ./output/group_consignment\n",
      "\n",
      "Evaluating IM model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "evaluate() got multiple values for argument 'variant'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxes_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m50\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m group_metrics = \u001b[43mprocess_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxes_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m all_results[xes_file] = group_metrics\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 153\u001b[39m, in \u001b[36mprocess_group\u001b[39m\u001b[34m(log_name, output_folder)\u001b[39m\n\u001b[32m    150\u001b[39m     visualize_models(process_tree, net, initial_marking, final_marking, variant_name, group_output_folder)\n\u001b[32m    152\u001b[39m     \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     metrics = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_marking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_marking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariant_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m     all_metrics.append(metrics)\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m all_metrics\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(log, net, initial_marking, final_marking, variant_name)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvariant_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# Compute fitness using token-based replay\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m fitness = \u001b[43mreplay_fitness\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_marking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_marking\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreplay_fitness\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVariants\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTOKEN_BASED\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# Compute precision using ETConformance\u001b[39;00m\n\u001b[32m     88\u001b[39m precision = precision_evaluator.evaluate(log, net, initial_marking, final_marking,\n\u001b[32m     89\u001b[39m                                        variant=precision_evaluator.Variants.ETCONFORMANCE_TOKEN)\n",
      "\u001b[31mTypeError\u001b[39m: evaluate() got multiple values for argument 'variant'"
     ]
    }
   ],
   "source": [
    "# Find all XES files in the data folder\n",
    "xes_files = [f for f in os.listdir(data_folder) if f.endswith('.xes') and f.startswith('group_')]\n",
    "print(f\"Found {len(xes_files)} XES files in {data_folder}:\")\n",
    "for f in xes_files:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "# Create output folder for visualizations\n",
    "output_folder = './output'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Process each group\n",
    "all_results = {}\n",
    "for xes_file in xes_files:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing {xes_file}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    group_metrics = process_group(xes_file, output_folder)\n",
    "    all_results[xes_file] = group_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7912d772",
   "metadata": {},
   "source": [
    "## 3. Compare Results\n",
    "\n",
    "Let's create a summary view to compare the performance of different inductive miner variants across the different process groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb71a89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a dataframe with all results\n",
    "results_df = []\n",
    "for log_name, metrics_list in all_results.items():\n",
    "    for metrics in metrics_list:\n",
    "        metrics_copy = metrics.copy()\n",
    "        metrics_copy['log'] = log_name.replace('.xes', '').replace('group_', '')\n",
    "        results_df.append(metrics_copy)\n",
    "\n",
    "results_df = pd.DataFrame(results_df)\n",
    "print(results_df)\n",
    "\n",
    "# Plot the results using a heatmap for each metric\n",
    "metrics = ['fitness', 'precision', 'generalization', 'simplicity']\n",
    "\n",
    "fig, axes = plt.subplots(len(metrics), 1, figsize=(12, 4*len(metrics)))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    pivot_df = results_df.pivot(index='log', columns='variant', values=metric)\n",
    "    sns.heatmap(pivot_df, annot=True, cmap='YlGnBu', ax=axes[i])\n",
    "    axes[i].set_title(f'{metric.capitalize()} by variant and log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05f1f30",
   "metadata": {},
   "source": [
    "## 4. Detailed Analysis of the \"2-way match\" Process\n",
    "\n",
    "Let's focus on the \"2-way match\" process group and analyze it in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e41c611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 2-way match log\n",
    "two_way_log = load_log(os.path.join(data_folder, 'group_2_way.xes'))\n",
    "\n",
    "# Get some basic statistics\n",
    "print(f\"Number of cases: {len(two_way_log)}\")\n",
    "print(f\"Number of events: {sum(len(case) for case in two_way_log)}\")\n",
    "print(f\"Average events per case: {sum(len(case) for case in two_way_log) / len(two_way_log):.2f}\")\n",
    "\n",
    "# Get the most frequent variants\n",
    "variants = pm4py.get_variants(two_way_log)\n",
    "print(f\"Number of variants: {len(variants)}\")\n",
    "\n",
    "# Display top 5 variants\n",
    "variant_count = [(count, trace) for trace, count in variants.items()]\n",
    "variant_count.sort(reverse=True)\n",
    "print(\"\\nTop 5 variants:\")\n",
    "for i, (count, trace) in enumerate(variant_count[:5]):\n",
    "    activities = [event['concept:name'] for event in trace]\n",
    "    print(f\"{i+1}. Count: {count} - Activities: {' → '.join(activities)}\")\n",
    "\n",
    "# Apply the IMf variant (usually good balance between precision and generalization)\n",
    "process_tree = inductive_miner.apply_tree(two_way_log, variant=inductive_miner.Variants.IMF)\n",
    "net, initial_marking, final_marking = pm4py.convert_to_petri_net(process_tree)\n",
    "\n",
    "# Visualize the process tree and Petri net\n",
    "pt_gviz = pt_visualizer.apply(process_tree)\n",
    "pt_visualizer.view(pt_gviz)\n",
    "\n",
    "# Perform more detailed conformance checking\n",
    "from pm4py.algo.conformance.tokenreplay import algorithm as token_replay\n",
    "replayed_traces = token_replay.apply(two_way_log, net, initial_marking, final_marking)\n",
    "\n",
    "# Analyze conformance results\n",
    "conforming_traces = [trace for trace in replayed_traces if trace['trace_is_fit']]\n",
    "print(f\"Conforming traces: {len(conforming_traces)} out of {len(replayed_traces)} ({len(conforming_traces)/len(replayed_traces)*100:.2f}%)\")\n",
    "\n",
    "# Identify problematic transitions if any\n",
    "all_transitions = set()\n",
    "activated_transitions = set()\n",
    "missing_transitions = set()\n",
    "\n",
    "for trace in replayed_traces:\n",
    "    for trans in trace['activated_transitions']:\n",
    "        all_transitions.add(trans.name)\n",
    "        activated_transitions.add(trans.name)\n",
    "    for trans in trace['missing_tokens_transitions']:\n",
    "        all_transitions.add(trans.name)\n",
    "        missing_transitions.add(trans.name)\n",
    "\n",
    "print(\"\\nActivated transitions:\", len(activated_transitions))\n",
    "print(\"Missing transitions:\", len(missing_transitions))\n",
    "if missing_transitions:\n",
    "    print(\"Problematic transitions:\", missing_transitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeb214f",
   "metadata": {},
   "source": [
    "## 5. Comparing Different Process Groups\n",
    "\n",
    "Let's now compare the different process groups to understand how they differ from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a080ab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the logs\n",
    "logs = {}\n",
    "for xes_file in xes_files:\n",
    "    group_name = xes_file.replace('.xes', '').replace('group_', '')\n",
    "    logs[group_name] = load_log(os.path.join(data_folder, xes_file))\n",
    "\n",
    "# Compare number of variants and activities\n",
    "for group_name, log in logs.items():\n",
    "    variants = pm4py.get_variants(log)\n",
    "    activities = pm4py.get_event_attribute_values(log, \"concept:name\")\n",
    "    \n",
    "    print(f\"\\nGroup: {group_name}\")\n",
    "    print(f\"  Cases: {len(log)}\")\n",
    "    print(f\"  Events: {sum(len(case) for case in log)}\")\n",
    "    print(f\"  Variants: {len(variants)}\")\n",
    "    print(f\"  Activities: {len(activities)}\")\n",
    "    print(f\"  Events per case: {sum(len(case) for case in log) / len(log):.2f}\")\n",
    "\n",
    "# Identify common and unique activities across groups\n",
    "all_activities = {}\n",
    "for group_name, log in logs.items():\n",
    "    activities = set(pm4py.get_event_attribute_values(log, \"concept:name\").keys())\n",
    "    all_activities[group_name] = activities\n",
    "\n",
    "# Find activities that appear in all groups\n",
    "if len(all_activities) > 1:\n",
    "    common_activities = set.intersection(*all_activities.values())\n",
    "    print(f\"\\nCommon activities across all groups ({len(common_activities)}):\")\n",
    "    print(\", \".join(sorted(common_activities)))\n",
    "    \n",
    "    # Find activities unique to each group\n",
    "    for group_name, activities in all_activities.items():\n",
    "        unique_activities = activities - set.union(*(set(acts) for g, acts in all_activities.items() if g != group_name))\n",
    "        print(f\"\\nActivities unique to {group_name} ({len(unique_activities)}):\")\n",
    "        if unique_activities:\n",
    "            print(\", \".join(sorted(unique_activities)))\n",
    "        else:\n",
    "            print(\"None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bc634b",
   "metadata": {},
   "source": [
    "## 6. Advanced Process Mining: Directly-Follows Graphs\n",
    "\n",
    "Let's visualize the directly-follows graphs (DFGs) for each process group, which can provide a simpler view of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9661acda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.visualization.dfg import visualizer as dfg_visualizer\n",
    "\n",
    "# For each log, create and visualize a directly-follows graph\n",
    "for group_name, log in logs.items():\n",
    "    print(f\"\\nGenerating DFG for {group_name}...\")\n",
    "    \n",
    "    # Discover directly-follows graph\n",
    "    dfg, start_activities, end_activities = pm4py.discover_directly_follows_graph(log)\n",
    "    \n",
    "    # Visualize DFG\n",
    "    parameters = {dfg_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: \"png\"}\n",
    "    gviz = dfg_visualizer.apply(dfg, start_activities, end_activities, parameters=parameters)\n",
    "    \n",
    "    # Save to file\n",
    "    output_path = os.path.join(output_folder, f\"dfg_{group_name}.png\")\n",
    "    dfg_visualizer.save(gviz, output_path)\n",
    "    \n",
    "    print(f\"DFG visualization saved to {output_path}\")\n",
    "    \n",
    "    # Just for display in the notebook\n",
    "    dfg_visualizer.view(gviz)\n",
    "\n",
    "    # Also generate a performance DFG to show time differences\n",
    "    perf_dfg, perf_sa, perf_ea = pm4py.discover_performance_directly_follows_graph(log)\n",
    "    \n",
    "    # Visualize performance DFG\n",
    "    perf_gviz = dfg_visualizer.apply(perf_dfg, perf_sa, perf_ea, variant=dfg_visualizer.Variants.PERFORMANCE, \n",
    "                                 parameters=parameters)\n",
    "    \n",
    "    # Save to file\n",
    "    perf_output_path = os.path.join(output_folder, f\"perf_dfg_{group_name}.png\")\n",
    "    dfg_visualizer.save(perf_gviz, perf_output_path)\n",
    "    \n",
    "    print(f\"Performance DFG visualization saved to {perf_output_path}\")\n",
    "    \n",
    "    # Just for display in the notebook\n",
    "    dfg_visualizer.view(perf_gviz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c5db6b",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1. Loaded and analyzed filtered XES logs for different process groups\n",
    "2. Used various inductive miner variants to discover process models\n",
    "3. Evaluated the quality of these models using standard metrics\n",
    "4. Compared different process groups\n",
    "5. Visualized models as process trees, Petri nets, and directly-follows graphs\n",
    "\n",
    "The different inductive miner variants offer different trade-offs:\n",
    "- IM (Inductive Miner): Balanced approach, guarantees sound models\n",
    "- IMf (Inductive Miner - infrequent): Filters out infrequent behavior\n",
    "- IMd (Inductive Miner - directly follows): Uses the directly-follows abstraction\n",
    "- IM_CLEAN: More aggressive filtering approach to create simpler models\n",
    "\n",
    "For further analysis, you might consider:\n",
    "- Analyzing the resource perspective (who performs which activities)\n",
    "- Analyzing the time perspective (bottlenecks, throughput times)\n",
    "- Building predictive models based on these discovered processes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
